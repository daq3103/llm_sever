{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/nnminh322/AI_Agent.git\n!apt-get update && apt-get -qq install -y nginx\n%cd AI_Agent/llm/serving\n!pip install uv\n!uv pip install -r requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:40:23.594256Z","iopub.execute_input":"2025-11-11T08:40:23.594974Z","iopub.status.idle":"2025-11-11T08:41:39.133925Z","shell.execute_reply.started":"2025-11-11T08:40:23.594948Z","shell.execute_reply":"2025-11-11T08:41:39.133174Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'AI_Agent'...\nremote: Enumerating objects: 8230, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (13/13), done.\u001b[K\nremote: Total 8230 (delta 4), reused 13 (delta 3), pack-reused 8214 (from 2)\u001b[K\nReceiving objects: 100% (8230/8230), 73.63 MiB | 25.23 MiB/s, done.\nResolving deltas: 100% (732/732), done.\nHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \nGet:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,125 kB]\nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,826 kB]\nHit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nGet:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\nGet:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,437 kB] \nHit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\nGet:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\nGet:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\nGet:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\nGet:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\nFetched 35.6 MB in 3s (12.5 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nPreconfiguring packages ...\nSelecting previously unselected package nginx-common.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../0-nginx-common_1.18.0-6ubuntu14.7_all.deb ...\nUnpacking nginx-common (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package libnginx-mod-http-geoip2.\nPreparing to unpack .../1-libnginx-mod-http-geoip2_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking libnginx-mod-http-geoip2 (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package libnginx-mod-http-image-filter.\nPreparing to unpack .../2-libnginx-mod-http-image-filter_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking libnginx-mod-http-image-filter (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package libnginx-mod-http-xslt-filter.\nPreparing to unpack .../3-libnginx-mod-http-xslt-filter_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking libnginx-mod-http-xslt-filter (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package libnginx-mod-mail.\nPreparing to unpack .../4-libnginx-mod-mail_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking libnginx-mod-mail (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package libnginx-mod-stream.\nPreparing to unpack .../5-libnginx-mod-stream_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking libnginx-mod-stream (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package libnginx-mod-stream-geoip2.\nPreparing to unpack .../6-libnginx-mod-stream-geoip2_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking libnginx-mod-stream-geoip2 (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package nginx-core.\nPreparing to unpack .../7-nginx-core_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking nginx-core (1.18.0-6ubuntu14.7) ...\nSelecting previously unselected package nginx.\nPreparing to unpack .../8-nginx_1.18.0-6ubuntu14.7_amd64.deb ...\nUnpacking nginx (1.18.0-6ubuntu14.7) ...\nSetting up nginx-common (1.18.0-6ubuntu14.7) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/nginx.service → /lib/systemd/system/nginx.service.\nSetting up libnginx-mod-http-xslt-filter (1.18.0-6ubuntu14.7) ...\nSetting up libnginx-mod-http-geoip2 (1.18.0-6ubuntu14.7) ...\nSetting up libnginx-mod-mail (1.18.0-6ubuntu14.7) ...\nSetting up libnginx-mod-http-image-filter (1.18.0-6ubuntu14.7) ...\nSetting up libnginx-mod-stream (1.18.0-6ubuntu14.7) ...\nSetting up libnginx-mod-stream-geoip2 (1.18.0-6ubuntu14.7) ...\nSetting up nginx-core (1.18.0-6ubuntu14.7) ...\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\nSetting up nginx (1.18.0-6ubuntu14.7) ...\nProcessing triggers for man-db (2.10.2-1) ...\n/kaggle/working/AI_Agent/llm/serving\nCollecting uv\n  Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: uv\nSuccessfully installed uv-0.9.8\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m154 packages\u001b[0m \u001b[2min 1.02s\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m58 packages\u001b[0m \u001b[2min 48.81s\u001b[0m\u001b[0m                                           \n\u001b[2mUninstalled \u001b[1m26 packages\u001b[0m \u001b[2min 2.02s\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m58 packages\u001b[0m \u001b[2min 419ms\u001b[0m\u001b[0m                              \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcbor2\u001b[0m\u001b[2m==5.7.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.11.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.19.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.116.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.118.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.16\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.3.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.7.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.3.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.11.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.8.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopenai-harmony\u001b[0m\u001b[2m==0.0.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.12.0.88\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.2.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpydantic-extra-types\u001b[0m\u001b[2m==2.10.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyngrok\u001b[0m\u001b[2m==7.4.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.15.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.7.6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msetproctitle\u001b[0m\u001b[2m==1.3.7\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.8.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cu124 (from https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.22.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.11.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.32.post1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.25\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import subprocess\nimport time\n\ncommand = [\"python\", \"serving.py\"]\n\nlog_file = open(\"server.log\", \"w\")\n\nprint(\"Khởi động server vllm...\")\n\nprocess = subprocess.Popen(\n    command,\n    stdout=log_file,\n    stderr=log_file\n)\n\nprint(f\"Server đã khởi động với PID: {process.pid}\")\nprint(\"Server đang chạy ở chế độ nền. Cell này đã hoàn thành.\")\n\ntime.sleep(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:41:55.237343Z","iopub.execute_input":"2025-11-11T08:41:55.237643Z","iopub.status.idle":"2025-11-11T08:42:00.244519Z","shell.execute_reply.started":"2025-11-11T08:41:55.237597Z","shell.execute_reply":"2025-11-11T08:42:00.243779Z"}},"outputs":[{"name":"stdout","text":"Khởi động server vllm...\nServer đã khởi động với PID: 1067\nServer đang chạy ở chế độ nền. Cell này đã hoàn thành.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!cat /kaggle/working/AI_Agent/llm/serving/server.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:42:06.198992Z","iopub.execute_input":"2025-11-11T08:42:06.199806Z","iopub.status.idle":"2025-11-11T08:42:06.317609Z","shell.execute_reply.started":"2025-11-11T08:42:06.199776Z","shell.execute_reply":"2025-11-11T08:42:06.316919Z"}},"outputs":[{"name":"stdout","text":"2025-11-11 08:41:55.383 | INFO     | __main__:run_vllm_server:33 - Khởi chạy server vLLM: /usr/bin/python3 -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-4B-Instruct-2507 --tensor-parallel-size 1 --dtype half --max-model-len 4096 --gpu-memory-utilization 0.8 --swap-space 8 --port 8000 --host 127.0.0.1\n2025-11-11 08:41:55.384 | INFO     | __main__:run_vllm_server:38 - Đang chờ server tại http://127.0.0.1:8000...\nINFO 11-11 08:42:03 [__init__.py:216] Automatically detected platform cuda.\n2025-11-11 08:42:05.939157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762850526.173136    1069 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Quay lại thư mục gốc (nơi chứa 'etc' và 'serving')\n%cd ..\n\nprint(\"--- 2. Cấu hình Nginx... ---\")\n\n# 1. Copy file config từ thư mục \"etc\" của bạn\n# (SỬA LẠI THÀNH \"sites-available\")\n!sudo cp ./etc/nginx/sites-available/llm_gateway.conf /etc/nginx/sites-available/\n\n# 2. Kích hoạt config này (tạo symlink)\n!sudo ln -s /etc/nginx/sites-available/llm_gateway.conf /etc/nginx/sites-enabled/\n\n# 3. Xóa file \"default\" của Nginx để tránh xung đột cổng 80\n!sudo rm -f /etc/nginx/sites-enabled/default\n\n# 4. Kiểm tra xem file config của bạn có lỗi không\n!sudo nginx -t\n\n# 5. Khởi động Nginx (dùng lệnh trực tiếp, vì Colab không có systemctl)\nprint(\"--- 3. Khởi động Nginx... ---\")\n!sudo nginx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:42:10.343180Z","iopub.execute_input":"2025-11-11T08:42:10.343477Z","iopub.status.idle":"2025-11-11T08:42:11.118764Z","shell.execute_reply.started":"2025-11-11T08:42:10.343445Z","shell.execute_reply":"2025-11-11T08:42:11.117889Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/AI_Agent/llm\n--- 2. Cấu hình Nginx... ---\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\n--- 3. Khởi động Nginx... ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pyngrok import ngrok\nimport getpass\n\nNGROK_AUTH_TOKEN = getpass.getpass(\"Nhập NGROK_AUTH_TOKEN của bạn: \")\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\npublic_url = ngrok.connect(80, \"http\")\nprint(f\"API của bạn đang chạy tại: {public_url}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:42:14.662495Z","iopub.execute_input":"2025-11-11T08:42:14.663064Z","iopub.status.idle":"2025-11-11T08:42:39.138743Z","shell.execute_reply.started":"2025-11-11T08:42:14.663030Z","shell.execute_reply":"2025-11-11T08:42:39.137965Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Nhập NGROK_AUTH_TOKEN của bạn:  ········\n"},{"name":"stdout","text":"API của bạn đang chạy tại: NgrokTunnel: \"https://298e4b19873f.ngrok-free.app\" -> \"http://localhost:80\"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!cat /kaggle/working/AI_Agent/llm/serving/server.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T08:49:13.820127Z","iopub.execute_input":"2025-11-11T08:49:13.820445Z","iopub.status.idle":"2025-11-11T08:49:13.940621Z","shell.execute_reply.started":"2025-11-11T08:49:13.820413Z","shell.execute_reply":"2025-11-11T08:49:13.939876Z"}},"outputs":[{"name":"stdout","text":"2025-11-11 08:41:55.383 | INFO     | __main__:run_vllm_server:33 - Khởi chạy server vLLM: /usr/bin/python3 -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-4B-Instruct-2507 --tensor-parallel-size 1 --dtype half --max-model-len 4096 --gpu-memory-utilization 0.8 --swap-space 8 --port 8000 --host 127.0.0.1\n2025-11-11 08:41:55.384 | INFO     | __main__:run_vllm_server:38 - Đang chờ server tại http://127.0.0.1:8000...\nINFO 11-11 08:42:03 [__init__.py:216] Automatically detected platform cuda.\n2025-11-11 08:42:05.939157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762850526.173136    1069 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762850526.245759    1069 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:42:25 [api_server.py:1839] vLLM API server version 0.11.0\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:42:25 [utils.py:233] non-default args: {'host': '127.0.0.1', 'model': 'Qwen/Qwen3-4B-Instruct-2507', 'dtype': 'half', 'max_model_len': 4096, 'gpu_memory_utilization': 0.8, 'swap_space': 8.0}\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:42:41 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m WARNING 11-11 08:42:41 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:42:41 [model.py:1510] Using max model len 4096\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:42:44 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 11-11 08:42:50 [__init__.py:216] Automatically detected platform cuda.\n2025-11-11 08:42:51.055584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762850571.077326    1150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762850571.086129    1150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:42:57 [core.py:644] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:42:57 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='Qwen/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m ERROR 11-11 08:42:59 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n[W1111 08:43:10.956648984 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W1111 08:43:20.967197459 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:43:20 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m WARNING 11-11 08:43:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:43:21 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-4B-Instruct-2507...\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:43:21 [gpu_model_runner.py:2634] Loading model from scratch...\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:43:21 [cuda.py:372] Using FlexAttention backend on V1 engine.\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:43:22 [weight_utils.py:392] Using model weights format ['*.safetensors']\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:43:49 [weight_utils.py:413] Time spent downloading weights for Qwen/Qwen3-4B-Instruct-2507: 27.830745 seconds\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:03<00:07,  3.93s/it]\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:08<00:04,  4.23s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:11<00:00,  3.54s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:11<00:00,  3.70s/it]\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m \n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:44:01 [default_loader.py:267] Loading weights took 11.10 seconds\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:44:01 [gpu_model_runner.py:2653] Model loading took 7.6065 GiB and 39.553269 seconds\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:44:15 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/1f6720e632/rank_0_0/backbone for vLLM's torch.compile\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:44:15 [backends.py:559] Dynamo bytecode transform time: 13.25 s\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m [rank0]:W1111 08:44:17.406000 1150 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:44:21 [backends.py:197] Cache the graph for dynamic shape for later use\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:44:50 [backends.py:218] Compiling a graph for dynamic shape takes 34.18 s\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:45:02 [monitor.py:34] torch.compile takes 47.43 s in total\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:45:04 [gpu_worker.py:298] Available KV cache memory: 2.75 GiB\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:45:04 [kv_cache_utils.py:1087] GPU KV cache size: 20,048 tokens\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:45:04 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 4.89x\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m WARNING 11-11 08:45:04 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:07<00:00,  8.71it/s]\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:45:13 [gpu_model_runner.py:3480] Graph capturing finished in 9 secs, took 0.56 GiB\n\u001b[1;36m(EngineCore_DP0 pid=1150)\u001b[0;0m INFO 11-11 08:45:13 [core.py:210] init engine (profile, create kv cache, warmup model) took 71.57 seconds\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:14 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1253\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:14 [api_server.py:1634] Supported_tasks: ['generate']\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m WARNING 11-11 08:45:14 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:14 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [api_server.py:1912] Starting vLLM API server 0 on http://127.0.0.1:8000\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:34] Available routes are:\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /health, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /load, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /ping, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /ping, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /tokenize, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /detokenize, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/models, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /version, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/responses, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/completions, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /pooling, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /classify, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /score, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/score, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /rerank, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v1/rerank, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /v2/rerank, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /invocations, Methods: POST\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:45:15 [launcher.py:42] Route: /metrics, Methods: GET\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     Started server process [1069]\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     Waiting for application startup.\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     Application startup complete.\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     103.199.76.132:0 - \"GET /docs HTTP/1.0\" 200 OK\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     103.199.76.132:0 - \"GET /openapi.json HTTP/1.0\" 200 OK\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     103.199.76.194:0 - \"GET /docs HTTP/1.0\" 200 OK\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     103.199.76.194:0 - \"GET /openapi.json HTTP/1.0\" 200 OK\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:47:45 [chat_utils.py:560] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:48:05 [loggers.py:127] Engine 000: Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:48:15 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:48:25 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO:     103.199.76.194:0 - \"POST /v1/chat/completions HTTP/1.0\" 200 OK\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:48:35 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n\u001b[1;36m(APIServer pid=1069)\u001b[0;0m INFO 11-11 08:48:45 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}